# inference.py

from typing import Tuple
import numpy as np
import torch
from env import TicTacToe
from model import PolicyValueNet
from mcts import Node, select_leaf, backpropagate, get_action_probs

def load_model(model_path: str) -> PolicyValueNet:
    """
    åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹æƒé‡

    å‚æ•°ï¼š
        model_path (str): æ¨¡å‹ä¿å­˜è·¯å¾„

    è¿”å›ï¼š
        net (PolicyValueNet): åŠ è½½åçš„æ¨¡å‹
    """
    net = PolicyValueNet()
    net.load_state_dict(torch.load(model_path))
    net.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    return net

def human_move(state: np.ndarray) -> int:
    """
    è·å–äººç±»ç©å®¶çš„è¾“å…¥åŠ¨ä½œ

    å‚æ•°ï¼š
        state (np.ndarray): å½“å‰æ£‹ç›˜çŠ¶æ€

    è¿”å›ï¼š
        action (int): ç©å®¶é€‰æ‹©çš„ä½ç½®
    """
    print("å½“å‰æ£‹ç›˜ï¼š")
    board = state.reshape(3, 3)
    for row in board:
        print(" | ".join(["X" if x == 1 else "O" if x == -1 else " " for x in row]))
    while True:
        try:
            action = int(input("è¯·è¾“å…¥ä½ è¦è½å­çš„ä½ç½® (0-8): "))
            if action in range(9) and state[action] == 0:
                return action
            else:
                print("æ— æ•ˆä½ç½®ï¼Œè¯·é‡æ–°è¾“å…¥ï¼")
        except ValueError:
            print("è¯·è¾“å…¥æ•°å­—ï¼")

def ai_move(state: np.ndarray, net: PolicyValueNet) -> Tuple[int, float]:
    """
    ä½¿ç”¨ MCTS è®© AI é€‰æ‹©åŠ¨ä½œ

    å‚æ•°ï¼š
        state (np.ndarray): å½“å‰æ£‹ç›˜çŠ¶æ€
        net (PolicyValueNet): è®­ç»ƒå¥½çš„æ¨¡å‹

    è¿”å›ï¼š
        action (int): AI é€‰æ‹©çš„åŠ¨ä½œ
        win_rate (float): AI èƒœç‡ä¼°è®¡
    """
    root = Node(state)
    root.expand(net)

    for _ in range(50):  # MCTS æœç´¢æ¬¡æ•°
        leaf = select_leaf(root, net)
        value = leaf.evaluate(net)
        backpropagate(leaf, value)

    probs = get_action_probs(root)
    action = np.argmax(probs)
    win_rate = root.children[action].mean_value if action in root.children else 0.0
    return action, win_rate

def play_against_ai(model_path: str):
    """
    ä¸è®­ç»ƒå¥½çš„ AI å¯¹å¼ˆ

    å‚æ•°ï¼š
        model_path (str): æ¨¡å‹è·¯å¾„
    """
    net = load_model(model_path)
    env = TicTacToe()
    state = env.reset()

    print("ä½ æ˜¯ Xï¼ˆAIæ˜¯ Oï¼‰ï¼Œè¯·å¼€å§‹ä½ çš„å›åˆï¼š")

    while True:
        # äººç±»å…ˆæ‰‹
        action = human_move(state)
        state, reward, done, _ = env.step(action)

        if done:
            print("ä½ èµ¢äº†ï¼ğŸ‰") if reward == 1 else print("å¹³å±€ã€‚ğŸ¤")
            break

        # AI å›åº”
        ai_action, win_rate = ai_move(state, net)
        print(f"AI è½å­äº {ai_action}ï¼Œä¼°è®¡èƒœç‡: {win_rate:.2f}")
        state, reward, done, _ = env.step(ai_action)

        if done:
            print("ä½ è¾“äº† ğŸ˜¢") if reward == -1 else print("å¹³å±€ã€‚ğŸ¤")
            break

if __name__ == "__main__":
    MODEL_PATH = "/mnt/data1/zhongrongmiao/lm/tictactoe_mcts_rl/policy_value_net.pth"
    play_against_ai(MODEL_PATH)